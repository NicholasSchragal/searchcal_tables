{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from astropy.table import QTable, Table\n",
    "import numpy as np\n",
    "import astropy.units as u\n",
    "import pandas as pd\n",
    "from astropy.coordinates import SkyCoord\n",
    "from pathlib import Path\n",
    "from glob import glob\n",
    "import datetime\n",
    "\n",
    "save = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['searchcal_tables/searchcal_output/del_leo.csv', 'searchcal_tables/searchcal_output/bet_tau.csv', 'searchcal_tables/searchcal_output/eta_uma.csv', 'searchcal_tables/searchcal_output/alf02_cvn.csv', 'searchcal_tables/searchcal_output/bet_cmi.csv', 'searchcal_tables/searchcal_output/bet_cma.csv', 'searchcal_tables/searchcal_output/bet_leo.csv', 'searchcal_tables/searchcal_output/alf_cep.csv', 'searchcal_tables/searchcal_output/bet_eri.csv', 'searchcal_tables/searchcal_output/alf_lep.csv', 'searchcal_tables/searchcal_output/bet_lib.csv', 'searchcal_tables/searchcal_output/rho_pup.csv', 'searchcal_tables/searchcal_output/eta_tau.csv', 'searchcal_tables/searchcal_output/bet_uma.csv', 'searchcal_tables/searchcal_output/gam_ori.csv', 'searchcal_tables/searchcal_output/del_crv.csv', 'searchcal_tables/searchcal_output/zet_aql.csv', 'searchcal_tables/searchcal_output/zet_oph.csv', 'searchcal_tables/searchcal_output/eps_uma.csv', 'searchcal_tables/searchcal_output/kap_ori.csv', 'searchcal_tables/searchcal_output/eps_ori.csv']\n"
     ]
    }
   ],
   "source": [
    "wanted_columns = [\n",
    "    \"SIMBAD\",\n",
    "    \"HD\",\n",
    "    \"HIP\",\n",
    "    \"SpType\",\n",
    "    \"RotVel\", # km/s, v sin(i)\n",
    "    \"V\",\n",
    "    \"H\",\n",
    "    \"K\",\n",
    "    \"LDD\",\n",
    "    \"e_LDD\",\n",
    "    \"e_LDD_rel\",\n",
    "    \"UD_V\",\n",
    "    \"UD_H\",\n",
    "    \"UD_K\",\n",
    "    \"vis2\",\n",
    "    \"vis2Err\",\n",
    "]\n",
    "\n",
    "optimal_dra = 0.5 * u.hourangle\n",
    "\n",
    "max_baseline = 330 * u.m\n",
    "obs_wavelength = 1.65 * u.um\n",
    "min_scale = ((obs_wavelength / max_baseline) * u.rad).to(u.mas)\n",
    "#print(min_scale)\n",
    "\n",
    "searchcal_out_table_dir = Path(\"./searchcal_output/\")\n",
    "sifted_table_dir = Path(\"./sifted_tables/\")\n",
    "sifted_table_pattern = sifted_table_dir / \"stem.ecsv\"\n",
    "\n",
    "searchcal_tables = glob(str(searchcal_out_table_dir/\"*.csv\"))\n",
    "print(searchcal_tables)\n",
    "\n",
    "wiki_wanted_columns = [ # Column name, unit, precision\n",
    "    [\"SIMBAD\", None, None],\n",
    "    [\"HD\", None, 0],\n",
    "    [\"HIP\", None, 0],\n",
    "    [\"Sep\", u.deg, 2],\n",
    "    [\"RASep\", u.hourangle, 2],\n",
    "    [\"DECSep\", u.deg, 2],\n",
    "    [\"OPTSep\", u.deg, 2],\n",
    "    [\"SpType\", None, None],\n",
    "    [\"RotVel\", None, 1], # km/s, v sin(i)\n",
    "    [\"V\", None, 2],\n",
    "    [\"H\", None, 2],\n",
    "    [\"K\", None, 2],\n",
    "    [\"LDD\", None, 3],\n",
    "    [\"e_LDD\", None, 3],\n",
    "    [\"e_LDD_rel\", None, 3],\n",
    "    [\"UD_V\", None, 3],\n",
    "    [\"UD_H\", None, 3],\n",
    "    [\"UD_K\", None, 3],\n",
    "    [\"vis2\", None, 3],\n",
    "    [\"vis2Err\", None, 3],\n",
    "    [\"Scores\", None, 1],    \n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for t in searchcal_tables:\n",
    "    df = pd.read_csv(t, comment = \"#\")\n",
    "    df.drop(df.columns[len(df.columns)-1], axis=1, inplace=True)\n",
    "\n",
    "    searchcal_table = Table(df.values, names = df.columns)\n",
    "    coords = []\n",
    "    dists = []\n",
    "    ra_dists = []\n",
    "    dec_dists = []\n",
    "    optimal_point_dists = []\n",
    "    row1 = searchcal_table[0]\n",
    "    row1_coord = SkyCoord(row1[\"RAJ2000\"], row1[\"DEJ2000\"], unit = (u.hourangle, u.deg))\n",
    "    optimal_points = [\n",
    "        SkyCoord(ra=row1_coord.ra - optimal_dra, dec=row1_coord.dec),\n",
    "        SkyCoord(ra=row1_coord.ra + optimal_dra, dec=row1_coord.dec),        \n",
    "    ]\n",
    "\n",
    "    for row in searchcal_table:\n",
    "        coord = SkyCoord(row[\"RAJ2000\"], row[\"DEJ2000\"], unit = (u.hourangle, u.deg))\n",
    "        coords.append(coord)\n",
    "        dist = row1_coord.separation(coord)\n",
    "        dists.append(dist)\n",
    "        ra_dist = (coord.ra - row1_coord.ra).to(u.hourangle)\n",
    "        ra_dists.append(ra_dist)\n",
    "        dec_dist = (coord.dec - row1_coord.dec).to(u.deg)\n",
    "        dec_dists.append(dec_dist)\n",
    "\n",
    "        if np.isclose(dist, 0):\n",
    "            optimal_point_dists.append(np.nan * u.deg)\n",
    "            continue\n",
    "        \n",
    "        optimal_seps = []\n",
    "        for p in optimal_points:\n",
    "            optimal_seps.append(p.separation(coord).to(u.deg).value)\n",
    "        optimal_point_dists.append(np.amin(optimal_seps) * u.deg)\n",
    "\n",
    "    eval_table = searchcal_table[wanted_columns]\n",
    "\n",
    "    eval_table.add_columns((coords, dists, ra_dists, dec_dists, optimal_point_dists), indexes = (3, 3, 3, 3, 3), names = (\"RADEC\", \"Sep\", \"RASep\", \"DECSep\", \"OPTSep\"))\n",
    "\n",
    "    cut_table = eval_table[eval_table['Sep'] <= 10 * u.deg]\n",
    "\n",
    "    new_HDs = []\n",
    "    for hd in cut_table['HD']:\n",
    "        new_HDs.append(\"{:0.0f}\".format(hd).strip(\".\"))\n",
    "    cut_table['HD'] = new_HDs\n",
    "\n",
    "    def CalibratorScore(row):\n",
    "        # Separation from optimal calibrator placement\n",
    "        ra_score = np.abs(optimal_dra.value - np.abs(row['RASep']))\n",
    "        ra_scale = 1.0\n",
    "        \n",
    "        optsep_score = 10 - row['OPTSep']\n",
    "        optsep_scale = 1.0\n",
    "\n",
    "        # Rotational Velocity Score\n",
    "        rotvel_score = float(row['RotVel'] < 200)\n",
    "        rotvel_scale = 1.0\n",
    "\n",
    "        # UDD Score\n",
    "        udd_score = float(row['UD_H'] < min_scale.to(u.mas).value)\n",
    "        udd_scale = 1.0\n",
    "\n",
    "        # LDD Score\n",
    "        ldd_score = float(row['LDD'] < min_scale.to(u.mas).value)\n",
    "        ldd_scale = 1.0\n",
    "\n",
    "        # Generate final score\n",
    "        score = 0\n",
    "        score += ra_score * ra_scale\n",
    "        score += optsep_score * optsep_scale\n",
    "        score += rotvel_score * rotvel_scale\n",
    "        score += udd_score * udd_scale\n",
    "        score += ldd_score * ldd_scale\n",
    "\n",
    "        return score\n",
    "\n",
    "    scores = []\n",
    "    score_breakdowns = []\n",
    "    for idx, row in enumerate(cut_table):\n",
    "        if idx == 0:\n",
    "            scores.append(np.inf)\n",
    "            continue\n",
    "        \n",
    "        score = CalibratorScore(row)\n",
    "        scores.append(score)\n",
    "\n",
    "    cut_table.add_column(scores, index = -1, name = \"Scores\")\n",
    "\n",
    "    cut_table.sort(\"Scores\", reverse = True)\n",
    "\n",
    "    outfname = Path(t).stem\n",
    "\n",
    "    outpath = sifted_table_pattern.with_stem(outfname)\n",
    "\n",
    "    if save:\n",
    "        cut_table.write(str(outpath), overwrite = True)\n",
    "\n",
    "    mkdown_out = cut_table.to_pandas().to_markdown(index = False)\n",
    "\n",
    "    mkdown_outpath = outpath.with_suffix(\".md\")\n",
    "\n",
    "    if save:\n",
    "        with open(mkdown_outpath, 'w') as f:\n",
    "            f.write(mkdown_out)\n",
    "\n",
    "    # The below was generated using GPT-4o and modified\n",
    "\n",
    "    # Function to convert DataFrame to MediaWiki table format\n",
    "    def is_float_like(value):\n",
    "        # Check if the value is a float or a numpy float type\n",
    "        if isinstance(value, float):\n",
    "            return True\n",
    "        elif isinstance(value, np.float32) or isinstance(value, np.float64):\n",
    "            return True\n",
    "        elif isinstance(value, np.generic):\n",
    "            # Check if it's a numpy scalar and not an integer\n",
    "            return not isinstance(value, (np.int32, np.int64, np.int8, np.int16, np.intc))\n",
    "        \n",
    "        return False\n",
    "    \n",
    "    def df_to_mediawiki(df):\n",
    "        # Start with the table header\n",
    "        mediawiki_table = \"== Interferometric Calibrator Table - CHARA ==\\n\"\n",
    "        mediawiki_table += \"{| class=\\\"wikitable\\\"\\n\"\n",
    "        \n",
    "        # Add the header row\n",
    "        unit_string = \"\\nUnits -\"\n",
    "        mediawiki_table += \"|+\\n\"\n",
    "        for c in wiki_wanted_columns:\n",
    "            mediawiki_table += \"!{}\\n\".format(c[0])\n",
    "            if c[1] is not None:\n",
    "                unit_string += \" {}: {}\".format(c[0], str(c[1]).strip())\n",
    "\n",
    "\n",
    "        # Add each row of the DataFrame\n",
    "        for index, row in df.iterrows():\n",
    "            mediawiki_table += \"|-\\n\"\n",
    "            wiki_wc_last = len(wiki_wanted_columns) - 1\n",
    "            for idx, c in enumerate(wiki_wanted_columns):\n",
    "                name = c[0]\n",
    "                prec = c[2]\n",
    "\n",
    "                value = row[name]\n",
    "                \n",
    "                if prec is None:\n",
    "                    valstr = str(value)\n",
    "                elif prec == 0:\n",
    "                    valstr = str(round(float(value), prec)).split(\".\")[0]\n",
    "                else:\n",
    "                    valstr = str(round(float(value), prec))\n",
    "                \n",
    "                \n",
    "\n",
    "                if idx != wiki_wc_last:\n",
    "                    mediawiki_table += \"| {} |\".format(valstr)\n",
    "                else:\n",
    "                    mediawiki_table += \"| {}\\n\".format(valstr)\n",
    "\n",
    "#            rowlen = len(row)\n",
    "#            for idx, value in enumerate(row):\n",
    "#                if idx == rowlen-1:\n",
    "#                    if is_float_like(value):\n",
    "#                        mediawiki_table += \"| \" + str(np.round(value, 3)) + \"\\n\"\n",
    "#                    else:\n",
    "#                        mediawiki_table += \"| \" + str(value) + \"\\n\"\n",
    "#                else:\n",
    "#                    if is_float_like(value):\n",
    "#                        mediawiki_table += \"| \" + str(np.round(value, 3)) + \" |\"\n",
    "#                    else:\n",
    "#                        mediawiki_table += \"| \" + str(value) + \" |\"\n",
    "        \n",
    "        # End the table\n",
    "        mediawiki_table += \"|}\"\n",
    "\n",
    "        generator_link = \"https://drive.google.com/file/d/16YFkWxVqMv6mvf3FU8qOdNuLHu1IMyWc/view?usp=sharing\"\n",
    "        generator_time = datetime.datetime.now(datetime.timezone.utc).strftime(\"%Y-%m-%d\")\n",
    "        mediawiki_table += \"\\n{}\".format(unit_string)\n",
    "        mediawiki_table += \"\\n\\nTable generated on {} using [{} SearchCal settings specified here].\".format(generator_time, generator_link)\n",
    "        \n",
    "        return mediawiki_table\n",
    "\n",
    "    # Convert the DataFrame to MediaWiki format\n",
    "    mediawiki_output = df_to_mediawiki(cut_table.to_pandas())\n",
    "    # Write the output to a text file\n",
    "    if not save:print(mediawiki_output)\n",
    "    if save:\n",
    "        with open(outpath.with_suffix(\".txt\"), 'w') as f:\n",
    "            f.write(mediawiki_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "exosims-latest",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
